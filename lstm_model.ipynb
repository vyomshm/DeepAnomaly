{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"http://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"fc6d6c64-79a2-4a7e-bc48-839d75db1fe4\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(global) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (window._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    window._bokeh_onload_callbacks = [];\n",
       "    window._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "\n",
       "  \n",
       "  if (typeof (window._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    window._bokeh_timeout = Date.now() + 5000;\n",
       "    window._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    if (window.Bokeh !== undefined) {\n",
       "      document.getElementById(\"fc6d6c64-79a2-4a7e-bc48-839d75db1fe4\").textContent = \"BokehJS successfully loaded.\";\n",
       "    } else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function run_callbacks() {\n",
       "    window._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    delete window._bokeh_onload_callbacks\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    window._bokeh_onload_callbacks.push(callback);\n",
       "    if (window._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    window._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        window._bokeh_is_loading--;\n",
       "        if (window._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"fc6d6c64-79a2-4a7e-bc48-839d75db1fe4\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid 'fc6d6c64-79a2-4a7e-bc48-839d75db1fe4' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "      document.getElementById(\"fc6d6c64-79a2-4a7e-bc48-839d75db1fe4\").textContent = \"BokehJS is loading...\";\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((window.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i](window.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < window._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!window._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      window._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"fc6d6c64-79a2-4a7e-bc48-839d75db1fe4\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (window._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(this));"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All dependencies imported!! TF: 1.0.1 ; Keras :2.0.1\n",
      "20 June 2017\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datetime import datetime,timedelta\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Activation,Dropout,Input\n",
    "from keras.layers.advanced_activations import LeakyReLU, PReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.recurrent import LSTM\n",
    "# from keras_tqdm import TQDMNotebookCallback\n",
    "# from ipywidgets import interact\n",
    "\n",
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "output_notebook()\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "print(\"All dependencies imported!! TF: {} ; Keras :{}\".format(tf.__version__,keras.__version__))\n",
    "\n",
    "!(date +%d\\ %B\\ %G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_data_stats(rucio_data):\n",
    "    sns.set_context('poster')\n",
    "    \n",
    "    ax = sns.countplot(x='activity',data= rucio_data)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation=90)\n",
    "    plt.show()\n",
    "    gx= sns.countplot(x='transfer-endpoint', data = rucio_data)\n",
    "    gx.set_xticklabels(gx.get_xticklabels(), rotation=30)\n",
    "    plt.show()\n",
    "    vx = sns.countplot(x='protocol', data = rucio_data)\n",
    "    plt.show()\n",
    "    bx= sns.countplot(x='src-type', data=rucio_data)\n",
    "    plt.show()\n",
    "    cx= sns.countplot(x='dst-type', data=rucio_data)\n",
    "    plt.show()\n",
    "    \n",
    "def preprocess_data(rucio_data):\n",
    "    \n",
    "    fields_to_drop = ['account','reason','checksum-adler','checksum-md5','guid','request-id','transfer-id','tool-id',\n",
    "                      'transfer-link','name','previous-request-id','src-url','dst-url', 'Unnamed: 0']\n",
    "    timestamps = ['started_at', 'submitted_at','transferred_at']\n",
    "\n",
    "    #DROP FIELDS , CHANGE TIME FORMAT\n",
    "    rucio_data = rucio_data.drop(fields_to_drop, axis=1)\n",
    "    for timestamp in timestamps:\n",
    "        rucio_data[timestamp]= pd.to_datetime(rucio_data[timestamp], infer_datetime_format=True)\n",
    "    rucio_data['delay'] = rucio_data['started_at'] - rucio_data['submitted_at']\n",
    "    rucio_data['delay'] = rucio_data['delay'].astype('timedelta64[s]')\n",
    "    \n",
    "    rucio_data = rucio_data.sort_values(by='submitted_at')\n",
    "\n",
    "    rucio_data = rucio_data.drop(timestamps, axis=1)\n",
    "    \n",
    "    src_encoder = LabelEncoder()\n",
    "    dst_encoder = LabelEncoder()\n",
    "    scope_encoder = LabelEncoder()\n",
    "    type_encoder = LabelEncoder()\n",
    "    activity_encoder = LabelEncoder()\n",
    "    protocol_encoder = LabelEncoder()\n",
    "    t_endpoint_encoder = LabelEncoder()\n",
    "\n",
    "    src_encoder.fit(rucio_data['src-rse'].unique())\n",
    "    dst_encoder.fit(rucio_data['dst-rse'].unique())\n",
    "    scope_encoder.fit(rucio_data['scope'].unique())\n",
    "    type_encoder.fit(rucio_data['src-type'].unique())\n",
    "    activity_encoder.fit(rucio_data['activity'].unique())\n",
    "    protocol_encoder.fit(rucio_data['protocol'].unique())\n",
    "    t_endpoint_encoder.fit(rucio_data['transfer-endpoint'].unique())\n",
    "\n",
    "    rucio_data['src-rse'] = src_encoder.transform(rucio_data['src-rse'])\n",
    "    rucio_data['dst-rse'] = dst_encoder.transform(rucio_data['dst-rse'])\n",
    "    rucio_data['scope'] = scope_encoder.transform(rucio_data['scope'])\n",
    "    rucio_data['src-type'] = type_encoder.transform(rucio_data['src-type'])\n",
    "    rucio_data['dst-type'] = type_encoder.transform(rucio_data['dst-type'])\n",
    "    rucio_data['activity'] = activity_encoder.transform(rucio_data['activity'])\n",
    "    rucio_data['protocol'] = protocol_encoder.transform(rucio_data['protocol'])\n",
    "    rucio_data['transfer-endpoint'] = t_endpoint_encoder.transform(rucio_data['transfer-endpoint'])\n",
    "    \n",
    "    return rucio_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>account</th>\n",
       "      <th>activity</th>\n",
       "      <th>bytes</th>\n",
       "      <th>checksum-adler</th>\n",
       "      <th>checksum-md5</th>\n",
       "      <th>dst-rse</th>\n",
       "      <th>dst-type</th>\n",
       "      <th>dst-url</th>\n",
       "      <th>duration</th>\n",
       "      <th>...</th>\n",
       "      <th>src-rse</th>\n",
       "      <th>src-type</th>\n",
       "      <th>src-url</th>\n",
       "      <th>started_at</th>\n",
       "      <th>submitted_at</th>\n",
       "      <th>tool-id</th>\n",
       "      <th>transfer-endpoint</th>\n",
       "      <th>transfer-id</th>\n",
       "      <th>transfer-link</th>\n",
       "      <th>transferred_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Production Input</td>\n",
       "      <td>533918</td>\n",
       "      <td>aadc03c6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CERN-PROD_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/a...</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>TOKYO-LCG2_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>srm://lcg-se01.icepp.jp:8446/srm/managerv2?SFN...</td>\n",
       "      <td>2017-05-28 07:00:38</td>\n",
       "      <td>2017-05-28 06:45:22</td>\n",
       "      <td>rucio-conveyor</td>\n",
       "      <td>https://fts3.cern.ch:8446</td>\n",
       "      <td>c9f91f7b-f91b-5df5-ae11-e9dcd249c244</td>\n",
       "      <td>https://fts3.cern.ch:8449/fts3/ftsmon/#/job/c9...</td>\n",
       "      <td>2017-05-28 07:00:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Production Input</td>\n",
       "      <td>591160</td>\n",
       "      <td>68fe0316</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CERN-PROD_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/a...</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>TOKYO-LCG2_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>srm://lcg-se01.icepp.jp:8446/srm/managerv2?SFN...</td>\n",
       "      <td>2017-05-28 07:03:02</td>\n",
       "      <td>2017-05-28 06:56:11</td>\n",
       "      <td>rucio-conveyor</td>\n",
       "      <td>https://fts3.cern.ch:8446</td>\n",
       "      <td>ff43c9e0-9716-5176-9c1e-3f0990f87745</td>\n",
       "      <td>https://fts3.cern.ch:8449/fts3/ftsmon/#/job/ff...</td>\n",
       "      <td>2017-05-28 07:03:11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Production Input</td>\n",
       "      <td>512581</td>\n",
       "      <td>3deb51ec</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CERN-PROD_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/a...</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>TOKYO-LCG2_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>srm://lcg-se01.icepp.jp:8446/srm/managerv2?SFN...</td>\n",
       "      <td>2017-05-28 06:58:12</td>\n",
       "      <td>2017-05-28 06:45:31</td>\n",
       "      <td>rucio-conveyor</td>\n",
       "      <td>https://fts3.cern.ch:8446</td>\n",
       "      <td>3d1d5437-700d-54a3-a0ba-46d0b3f11360</td>\n",
       "      <td>https://fts3.cern.ch:8449/fts3/ftsmon/#/job/3d...</td>\n",
       "      <td>2017-05-28 06:58:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Production Input</td>\n",
       "      <td>478343</td>\n",
       "      <td>62736ced</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CERN-PROD_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/a...</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>TOKYO-LCG2_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>srm://lcg-se01.icepp.jp:8446/srm/managerv2?SFN...</td>\n",
       "      <td>2017-05-28 07:02:51</td>\n",
       "      <td>2017-05-28 06:56:11</td>\n",
       "      <td>rucio-conveyor</td>\n",
       "      <td>https://fts3.cern.ch:8446</td>\n",
       "      <td>ff43c9e0-9716-5176-9c1e-3f0990f87745</td>\n",
       "      <td>https://fts3.cern.ch:8449/fts3/ftsmon/#/job/ff...</td>\n",
       "      <td>2017-05-28 07:02:59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Production Input</td>\n",
       "      <td>513541</td>\n",
       "      <td>965123c5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NET2_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>srm://atlas.bu.edu:8443/srm/v2/server?SFN=/gpf...</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>CERN-PROD_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/a...</td>\n",
       "      <td>2017-05-28 07:02:10</td>\n",
       "      <td>2017-05-28 06:56:06</td>\n",
       "      <td>rucio-conveyor</td>\n",
       "      <td>https://fts.usatlas.bnl.gov:8446</td>\n",
       "      <td>853cf990-478d-58b6-8669-d8a3396efd11</td>\n",
       "      <td>https://fts.usatlas.bnl.gov:8449/fts3/ftsmon/#...</td>\n",
       "      <td>2017-05-28 07:02:13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Production Input</td>\n",
       "      <td>540985</td>\n",
       "      <td>8f11a53a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CERN-PROD_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/a...</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>TOKYO-LCG2_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>srm://lcg-se01.icepp.jp:8446/srm/managerv2?SFN...</td>\n",
       "      <td>2017-05-28 07:01:43</td>\n",
       "      <td>2017-05-28 06:45:34</td>\n",
       "      <td>rucio-conveyor</td>\n",
       "      <td>https://fts3.cern.ch:8446</td>\n",
       "      <td>934b88e7-2ec3-552a-80c6-7f0d1822d175</td>\n",
       "      <td>https://fts3.cern.ch:8449/fts3/ftsmon/#/job/93...</td>\n",
       "      <td>2017-05-28 07:01:52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Production Input</td>\n",
       "      <td>458226</td>\n",
       "      <td>eb1f491b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CA-VICTORIA-WESTGRID-T2_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>srm://charon01.westgrid.ca:8443/srm/managerv2?...</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>INFN-T1_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>srm://storm-fe.cr.cnaf.infn.it:8444/srm/manage...</td>\n",
       "      <td>2017-05-28 06:59:24</td>\n",
       "      <td>2017-05-28 06:56:07</td>\n",
       "      <td>rucio-conveyor</td>\n",
       "      <td>https://fts.usatlas.bnl.gov:8446</td>\n",
       "      <td>fbf37172-2014-5a55-84da-a52af118bd2a</td>\n",
       "      <td>https://fts.usatlas.bnl.gov:8449/fts3/ftsmon/#...</td>\n",
       "      <td>2017-05-28 06:59:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Production Input</td>\n",
       "      <td>531690</td>\n",
       "      <td>114fc3a3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NET2_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>srm://atlas.bu.edu:8443/srm/v2/server?SFN=/gpf...</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>IFIC-LCG2_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>srm://srmv2.ific.uv.es:8443/srm/managerv2?SFN=...</td>\n",
       "      <td>2017-05-28 07:01:52</td>\n",
       "      <td>2017-05-28 06:56:09</td>\n",
       "      <td>rucio-conveyor</td>\n",
       "      <td>https://fts.usatlas.bnl.gov:8446</td>\n",
       "      <td>a0a67cc1-eafe-5e5c-abc6-041e1061ec33</td>\n",
       "      <td>https://fts.usatlas.bnl.gov:8449/fts3/ftsmon/#...</td>\n",
       "      <td>2017-05-28 07:02:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Production Input</td>\n",
       "      <td>527060</td>\n",
       "      <td>c920ba10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>CERN-PROD_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/a...</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>TOKYO-LCG2_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>srm://lcg-se01.icepp.jp:8446/srm/managerv2?SFN...</td>\n",
       "      <td>2017-05-28 06:58:09</td>\n",
       "      <td>2017-05-28 06:56:11</td>\n",
       "      <td>rucio-conveyor</td>\n",
       "      <td>https://fts3.cern.ch:8446</td>\n",
       "      <td>19f8a214-b283-5e0e-ba9b-551cd5004f55</td>\n",
       "      <td>https://fts3.cern.ch:8449/fts3/ftsmon/#/job/19...</td>\n",
       "      <td>2017-05-28 06:58:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Production Input</td>\n",
       "      <td>478330207</td>\n",
       "      <td>191570e4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RRC-KI-T1_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>srm://sdrm.t1.grid.kiae.ru:8443/srm/managerv2?...</td>\n",
       "      <td>27</td>\n",
       "      <td>...</td>\n",
       "      <td>MWT2_DATADISK</td>\n",
       "      <td>DISK</td>\n",
       "      <td>srm://uct2-dc1.uchicago.edu:8443/srm/managerv2...</td>\n",
       "      <td>2017-05-28 07:01:58</td>\n",
       "      <td>2017-05-28 05:16:40</td>\n",
       "      <td>rucio-conveyor</td>\n",
       "      <td>https://fts.usatlas.bnl.gov:8446</td>\n",
       "      <td>fa222ff4-c5d6-52bf-aa2b-85e1dd058474</td>\n",
       "      <td>https://fts.usatlas.bnl.gov:8449/fts3/ftsmon/#...</td>\n",
       "      <td>2017-05-28 07:02:25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  account          activity      bytes checksum-adler  \\\n",
       "0           0      NaN  Production Input     533918       aadc03c6   \n",
       "1           1      NaN  Production Input     591160       68fe0316   \n",
       "2           2      NaN  Production Input     512581       3deb51ec   \n",
       "3           3      NaN  Production Input     478343       62736ced   \n",
       "4           4      NaN  Production Input     513541       965123c5   \n",
       "5           5      NaN  Production Input     540985       8f11a53a   \n",
       "6           6      NaN  Production Input     458226       eb1f491b   \n",
       "7           7      NaN  Production Input     531690       114fc3a3   \n",
       "8           8      NaN  Production Input     527060       c920ba10   \n",
       "9           9      NaN  Production Input  478330207       191570e4   \n",
       "\n",
       "   checksum-md5                           dst-rse dst-type  \\\n",
       "0           NaN                CERN-PROD_DATADISK     DISK   \n",
       "1           NaN                CERN-PROD_DATADISK     DISK   \n",
       "2           NaN                CERN-PROD_DATADISK     DISK   \n",
       "3           NaN                CERN-PROD_DATADISK     DISK   \n",
       "4           NaN                     NET2_DATADISK     DISK   \n",
       "5           NaN                CERN-PROD_DATADISK     DISK   \n",
       "6           NaN  CA-VICTORIA-WESTGRID-T2_DATADISK     DISK   \n",
       "7           NaN                     NET2_DATADISK     DISK   \n",
       "8           NaN                CERN-PROD_DATADISK     DISK   \n",
       "9           NaN                RRC-KI-T1_DATADISK     DISK   \n",
       "\n",
       "                                             dst-url  duration  \\\n",
       "0  gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/a...        10   \n",
       "1  gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/a...         9   \n",
       "2  gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/a...         9   \n",
       "3  gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/a...         8   \n",
       "4  srm://atlas.bu.edu:8443/srm/v2/server?SFN=/gpf...         3   \n",
       "5  gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/a...         9   \n",
       "6  srm://charon01.westgrid.ca:8443/srm/managerv2?...         4   \n",
       "7  srm://atlas.bu.edu:8443/srm/v2/server?SFN=/gpf...         8   \n",
       "8  gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/a...         9   \n",
       "9  srm://sdrm.t1.grid.kiae.ru:8443/srm/managerv2?...        27   \n",
       "\n",
       "          ...                       src-rse src-type  \\\n",
       "0         ...           TOKYO-LCG2_DATADISK     DISK   \n",
       "1         ...           TOKYO-LCG2_DATADISK     DISK   \n",
       "2         ...           TOKYO-LCG2_DATADISK     DISK   \n",
       "3         ...           TOKYO-LCG2_DATADISK     DISK   \n",
       "4         ...            CERN-PROD_DATADISK     DISK   \n",
       "5         ...           TOKYO-LCG2_DATADISK     DISK   \n",
       "6         ...              INFN-T1_DATADISK     DISK   \n",
       "7         ...            IFIC-LCG2_DATADISK     DISK   \n",
       "8         ...           TOKYO-LCG2_DATADISK     DISK   \n",
       "9         ...                 MWT2_DATADISK     DISK   \n",
       "\n",
       "                                             src-url           started_at  \\\n",
       "0  srm://lcg-se01.icepp.jp:8446/srm/managerv2?SFN...  2017-05-28 07:00:38   \n",
       "1  srm://lcg-se01.icepp.jp:8446/srm/managerv2?SFN...  2017-05-28 07:03:02   \n",
       "2  srm://lcg-se01.icepp.jp:8446/srm/managerv2?SFN...  2017-05-28 06:58:12   \n",
       "3  srm://lcg-se01.icepp.jp:8446/srm/managerv2?SFN...  2017-05-28 07:02:51   \n",
       "4  gsiftp://eosatlassftp.cern.ch:2811/eos/atlas/a...  2017-05-28 07:02:10   \n",
       "5  srm://lcg-se01.icepp.jp:8446/srm/managerv2?SFN...  2017-05-28 07:01:43   \n",
       "6  srm://storm-fe.cr.cnaf.infn.it:8444/srm/manage...  2017-05-28 06:59:24   \n",
       "7  srm://srmv2.ific.uv.es:8443/srm/managerv2?SFN=...  2017-05-28 07:01:52   \n",
       "8  srm://lcg-se01.icepp.jp:8446/srm/managerv2?SFN...  2017-05-28 06:58:09   \n",
       "9  srm://uct2-dc1.uchicago.edu:8443/srm/managerv2...  2017-05-28 07:01:58   \n",
       "\n",
       "          submitted_at         tool-id                 transfer-endpoint  \\\n",
       "0  2017-05-28 06:45:22  rucio-conveyor         https://fts3.cern.ch:8446   \n",
       "1  2017-05-28 06:56:11  rucio-conveyor         https://fts3.cern.ch:8446   \n",
       "2  2017-05-28 06:45:31  rucio-conveyor         https://fts3.cern.ch:8446   \n",
       "3  2017-05-28 06:56:11  rucio-conveyor         https://fts3.cern.ch:8446   \n",
       "4  2017-05-28 06:56:06  rucio-conveyor  https://fts.usatlas.bnl.gov:8446   \n",
       "5  2017-05-28 06:45:34  rucio-conveyor         https://fts3.cern.ch:8446   \n",
       "6  2017-05-28 06:56:07  rucio-conveyor  https://fts.usatlas.bnl.gov:8446   \n",
       "7  2017-05-28 06:56:09  rucio-conveyor  https://fts.usatlas.bnl.gov:8446   \n",
       "8  2017-05-28 06:56:11  rucio-conveyor         https://fts3.cern.ch:8446   \n",
       "9  2017-05-28 05:16:40  rucio-conveyor  https://fts.usatlas.bnl.gov:8446   \n",
       "\n",
       "                            transfer-id  \\\n",
       "0  c9f91f7b-f91b-5df5-ae11-e9dcd249c244   \n",
       "1  ff43c9e0-9716-5176-9c1e-3f0990f87745   \n",
       "2  3d1d5437-700d-54a3-a0ba-46d0b3f11360   \n",
       "3  ff43c9e0-9716-5176-9c1e-3f0990f87745   \n",
       "4  853cf990-478d-58b6-8669-d8a3396efd11   \n",
       "5  934b88e7-2ec3-552a-80c6-7f0d1822d175   \n",
       "6  fbf37172-2014-5a55-84da-a52af118bd2a   \n",
       "7  a0a67cc1-eafe-5e5c-abc6-041e1061ec33   \n",
       "8  19f8a214-b283-5e0e-ba9b-551cd5004f55   \n",
       "9  fa222ff4-c5d6-52bf-aa2b-85e1dd058474   \n",
       "\n",
       "                                       transfer-link       transferred_at  \n",
       "0  https://fts3.cern.ch:8449/fts3/ftsmon/#/job/c9...  2017-05-28 07:00:48  \n",
       "1  https://fts3.cern.ch:8449/fts3/ftsmon/#/job/ff...  2017-05-28 07:03:11  \n",
       "2  https://fts3.cern.ch:8449/fts3/ftsmon/#/job/3d...  2017-05-28 06:58:21  \n",
       "3  https://fts3.cern.ch:8449/fts3/ftsmon/#/job/ff...  2017-05-28 07:02:59  \n",
       "4  https://fts.usatlas.bnl.gov:8449/fts3/ftsmon/#...  2017-05-28 07:02:13  \n",
       "5  https://fts3.cern.ch:8449/fts3/ftsmon/#/job/93...  2017-05-28 07:01:52  \n",
       "6  https://fts.usatlas.bnl.gov:8449/fts3/ftsmon/#...  2017-05-28 06:59:28  \n",
       "7  https://fts.usatlas.bnl.gov:8449/fts3/ftsmon/#...  2017-05-28 07:02:00  \n",
       "8  https://fts3.cern.ch:8449/fts3/ftsmon/#/job/19...  2017-05-28 06:58:18  \n",
       "9  https://fts.usatlas.bnl.gov:8449/fts3/ftsmon/#...  2017-05-28 07:02:25  \n",
       "\n",
       "[10 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rucio_data = pd.read_csv('may.csv')\n",
    "rucio_data = rucio_data[0:30000]\n",
    "rucio_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rucio_data = preprocess_data(rucio_data)\n",
    "durations = rucio_data['duration']\n",
    "rucio_data = rucio_data.drop(['duration'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 10) (30000,)\n"
     ]
    }
   ],
   "source": [
    "print(rucio_data.shape, durations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_and_preprocess_data(path='may.csv'):\n",
    "    \n",
    "    rucio_data = pd.read_csv(path)\n",
    "    rucio_data = rucio_data[0:30000]\n",
    "    rucio_data = preprocess_data(rucio_data)\n",
    "    durations = rucio_data['duration']\n",
    "    rucio_data = rucio_data.drop(['duration'], axis=1)\n",
    "    inputs = rucio_data.as_matrix()\n",
    "    outputs = durations.as_matrix()\n",
    "    print(inputs.shape, outputs.shape)\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# splitting data into test and training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_data(rucio_data,durations, batch_size=512, num_timesteps=1, split_frac=0.9):\n",
    "    \n",
    "    n_batches = int(rucio_data.shape[0] / batch_size)\n",
    "    \n",
    "    rucio_data = rucio_data[0:n_batches*batch_size]\n",
    "    durations = durations[0:n_batches*batch_size]\n",
    "    \n",
    "    x = np.stack(np.split(rucio_data, batch_size))\n",
    "    y = np.stack(np.split(durations, batch_size))\n",
    "    \n",
    "    print(x.shape, y.shape)\n",
    "    \n",
    "    split_idx = int(batch_size*split_frac)\n",
    "    trainX, trainY = x[:split_idx], y[:split_idx]\n",
    "    testX, testY = x[split_idx:], y[split_idx:]\n",
    "#     print(trainX.shape)\n",
    "    return trainX, trainY, testX, testY\n",
    "\n",
    "# trainX, trainY, testX, testY = split_data(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    layers = [10, 10, 1]\n",
    "    \n",
    "    model.add(LSTM(layers[0], input_shape=(None, 10), return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(LSTM(layers[1], return_sequences=False))\n",
    "    model.add(Dropout(0.4))\n",
    "    \n",
    "    model.add(Dense(layers[2]))\n",
    "    model.add(Activation(\"linear\"))\n",
    "    \n",
    "    start = time.time()\n",
    "    model.compile(loss=\"mse\", optimizer=\"rmsprop\")\n",
    "    print (\"Compilation Time : \", time.time() - start)\n",
    "    return model\n",
    "\n",
    "import keras.callbacks as cb\n",
    "\n",
    "class LossHistory(cb.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        batch_loss = logs.get('loss')\n",
    "        self.losses.append(batch_loss)\n",
    "\n",
    "\n",
    "\n",
    "# def build_model(n_steps):\n",
    "    \n",
    "#     layers = [10, 10, 1]\n",
    "#     model_inputs = Input(shape=[None,n_steps, 10])\n",
    "    \n",
    "#     layer_1 = LSTM(layers[0], return_sequences=True)(model_inputs)\n",
    "#     layer_2 = LSTM(layers[1], return_sequences=False)(layer_1)\n",
    "    \n",
    "#     model_output = Dense(layer[2], activation='linear')\n",
    "    \n",
    "#     model = Model(input=model_inputs, output=model_output)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def run_network(model=None,data=None, epochs=1, batch=128):\n",
    "    \n",
    "    print('\\n Loading data...')\n",
    "    if data is None:\n",
    "        rucio_data, durations = get_and_preprocess_data()\n",
    "\n",
    "        print('\\n Data Loaded and preprocesses!!....')\n",
    "        print('\\n Moving on to splitting and reshaping data...')\n",
    "#         trainX, trainY, testX, testY = split_data(inputs, outputs,batch_size=512, split_frac=0.9)\n",
    "        print('\\n Data split into train and test sets.. ')\n",
    "    else:\n",
    "        trainX, trainY, testX, testY = data\n",
    "    \n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if model is None:\n",
    "            model = build_model()\n",
    "\n",
    "            history = LossHistory()\n",
    "\n",
    "            print('Training model...')\n",
    "            training = model.fit(trainX, trainY, epochs=epochs, batch_size=batch,\n",
    "                                 validation_split=0.1, callbacks=[history], verbose=1)\n",
    "\n",
    "            print(\"Training duration : {0}\".format(time.time() - start_time))\n",
    "            score = model.evaluate(trainX, trainY, verbose=0)\n",
    "\n",
    "            print(\"Network's training score [MSE]: {0}\".format(score))\n",
    "            print(\"Training finished !!!!!!\")\n",
    "            return training, data, model, history.losses\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print('KeyboardInterrupt')\n",
    "        return model, history.losses\n",
    "    \n",
    "def plot_losses(losses):\n",
    "    sns.set_context('poster')\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.plot(losses)\n",
    "    ax.set_title('Loss per batch')\n",
    "    fig.show()\n",
    "    print(len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loading data...\n",
      "(30000, 10) (30000,)\n",
      "\n",
      " Data Loaded and preprocesses!!....\n",
      "\n",
      " Moving on to splitting and reshaping data...\n",
      "(512, 58, 10) (512, 58)\n",
      "\n",
      " Data split into train and test sets.. \n",
      "Compilation Time :  0.031991004943847656\n",
      "Training model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: expected activation_3 to have shape (None, 1) but got array with shape (460, 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-62231a8b91a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplot_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-37c726691eb1>\u001b[0m in \u001b[0;36mrun_network\u001b[0;34m(model, data, epochs, batch)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             training = model.fit(trainX, trainY, epochs=epochs, batch_size=batch,\n\u001b[0;32m---> 26\u001b[0;31m                                  validation_split=0.1, callbacks=[history], verbose=1)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training duration : {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vyom/anaconda3/envs/CERN/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 845\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/vyom/anaconda3/envs/CERN/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1403\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1405\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1406\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vyom/anaconda3/envs/CERN/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1297\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1299\u001b[0;31m                                     exception_prefix='model target')\n\u001b[0m\u001b[1;32m   1300\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1301\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/home/vyom/anaconda3/envs/CERN/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    131\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: expected activation_3 to have shape (None, 1) but got array with shape (460, 58)"
     ]
    }
   ],
   "source": [
    "training, data, model, losses = run_network()\n",
    "plot_losses(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
